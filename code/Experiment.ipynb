{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fourrooms import Fourrooms\n",
    "from IPython.display import clear_output\n",
    "from aoaoc_classes import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the command line argparse\n",
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        # Numbers\n",
    "        self.nepisodes=4000\n",
    "        self.nruns=1\n",
    "        self.nsteps=2000\n",
    "        self.noptions=3\n",
    "        \n",
    "        # Learning Rates\n",
    "        self.lr_term=0.1\n",
    "        self.lr_intra=0.25\n",
    "        self.lr_critic=0.5\n",
    "        self.lr_critic_pseudo=0.5\n",
    "        self.lr_criticA=0.5\n",
    "        self.lr_criticA_pseudo=0.5\n",
    "        self.lr_attend=0.02\n",
    "        \n",
    "        # Environment Parameters\n",
    "        self.discount=0.99\n",
    "        self.deterministic = False\n",
    "        self.punishEachStep = False\n",
    "        \n",
    "        # Attention Parameters\n",
    "        self.h_learn=False\n",
    "        self.clipthres = 0.1\n",
    "        self.stretchthres = 1.\n",
    "        self.stretchstep = 1.\n",
    "        \n",
    "        # Distraction Parameters\n",
    "        self.xi=1.\n",
    "        self.n=0.5\n",
    "        \n",
    "        # Policy Parameters\n",
    "        self.epsilon=1e-1\n",
    "        self.temperature=1.\n",
    "        \n",
    "        # Objective Parameters\n",
    "        self.wo1 = 1.   #q\n",
    "        self.wo2 = 2.    #cosim\n",
    "        self.wo3 = 2.    #entropy\n",
    "        self.wo4 = 5.    #size\n",
    "        self.wo4p = 2\n",
    "        \n",
    "        # Randomness Parameters\n",
    "        self.seed=2222\n",
    "        self.seed_startstate=1111\n",
    "        \n",
    "        # Display Parameters\n",
    "        \n",
    "        \n",
    "        # Other Parameters\n",
    "        self.baseline=True\n",
    "        self.dc = 0.1\n",
    "        \n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(args.seed)\n",
    "env = Fourrooms(args.seed_startstate, args.punishEachStep, args.deterministic)\n",
    "R = 50.\n",
    "\n",
    "possible_next_goals = [68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103]\n",
    "\n",
    "features = Tabular(env.observation_space.n)\n",
    "nfeatures, nactions = len(features), env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(args.nruns):\n",
    "    # Set up classes\n",
    "    policy_over_options = POO(rng, nfeatures, args, R)\n",
    "    CoSimObj.reset()\n",
    "    options = [Option(rng, nfeatures, nactions, args, R, policy_over_options, i) for i in range(args.noptions)]\n",
    "\n",
    "    # Loop through games\n",
    "    for episode in range(args.nepisodes):\n",
    "        # Initial state\n",
    "        return_per_episode = 0.0\n",
    "        observation = env.reset()\n",
    "        phi = features(observation)    \n",
    "        option = policy_over_options.sample(phi)\n",
    "        action = options[option].sample(phi)\n",
    "        traject = [[phi,option],[phi,option],action]\n",
    "        \n",
    "        # Reset record\n",
    "        cumreward = 0.\n",
    "        duration = 1\n",
    "        option_switches = 0\n",
    "        avgduration = 0.\n",
    "        \n",
    "        # Loop through frames in 1 game\n",
    "        for step in range(args.nsteps):\n",
    "            # Collect feedback from environment\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            phi = features(observation)\n",
    "            return_per_episode += pow(args.discount,step)*reward\n",
    "            \n",
    "            # Store option index\n",
    "            last_option = option\n",
    "            \n",
    "            # Check termination\n",
    "            termination = options[option].terminate(phi, value=True)\n",
    "            if options[option].terminate(phi):\n",
    "                option = policy_over_options.sample(phi)\n",
    "                option_switches += 1\n",
    "                avgduration += (1./option_switches)*(duration - avgduration)\n",
    "                duration = 1\n",
    "        \n",
    "            # Record into trajectory\n",
    "            traject[0] = traject[1]\n",
    "            traject[1] = [phi, option]\n",
    "            traject[2] = action\n",
    "            \n",
    "            # Sample next action\n",
    "            action = options[option].sample(phi)\n",
    "\n",
    "            # Policy Evaluation + Policy Improvement\n",
    "            options[last_option].update(traject, reward, done, phi, last_option, termination)\n",
    "            policy_over_options.update(traject, reward, options[last_option].distract(reward,traject[2]), done, termination)\n",
    "            \n",
    "            # End of frame\n",
    "            cumreward += options[last_option].distract(reward, traject[2])\n",
    "            duration += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print('Run {} episode {} steps {} cumreward {} avg. duration {} switches {}'.format(run, episode, step, cumreward, avgduration, option_switches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
